<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<!-- saved from url=(0087)http://www.ffconsultancy.com/products/fsharp_journal/subscribers/parallel_scimark2.html -->
<!--?xml version="1.0" encoding="iso-8859-1"?--><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd"><HTML><HEAD><META 
content="IE=10.000" http-equiv="X-UA-Compatible">
   <TITLE>F# Journal: Parallelizing the SciMark2 benchmark: part 1</TITLE>   
<META http-equiv="Content-Type" content="text/html; charset=windows-1252"><LINK 
href="F%23%20Journal%20Parallelizing%20the%20SciMark2%20benchmark%20part%201_files/style.css" 
rel="stylesheet" type="text/css">   <LINK href="F%23%20Journal%20Parallelizing%20the%20SciMark2%20benchmark%20part%201_files/style(1).css" 
rel="stylesheet" type="text/css">   <LINK href="F%23%20Journal%20Parallelizing%20the%20SciMark2%20benchmark%20part%201_files/individual.css" 
rel="stylesheet" type="text/css"> 
<META name="GENERATOR" content="MSHTML 10.00.9200.16458"></HEAD>
<BODY>       <TITLE>F# Journal: Parallelizing the SciMark2 benchmark: part 
1</TITLE>     <LINK href="F%23%20Journal%20Parallelizing%20the%20SciMark2%20benchmark%20part%201_files/style.css" 
rel="stylesheet" type="text/css">     <LINK href="F%23%20Journal%20Parallelizing%20the%20SciMark2%20benchmark%20part%201_files/style(1).css" 
rel="stylesheet" type="text/css">     <LINK href="F%23%20Journal%20Parallelizing%20the%20SciMark2%20benchmark%20part%201_files/individual.css" 
rel="stylesheet" type="text/css">         
<TABLE id="logo">
  <TBODY>
  <TR>
    <TD width="100%"><IMG src="F%23%20Journal%20Parallelizing%20the%20SciMark2%20benchmark%20part%201_files/title.gif"> 
              </TD>
    <TD><IMG src="F%23%20Journal%20Parallelizing%20the%20SciMark2%20benchmark%20part%201_files/left.gif"> 
              </TD></TR></TBODY></TABLE>
<TABLE id="menu">
  <TBODY>
  <TR>
    <TD width="25%">
    <TD width="25%"><A 
      href="http://www.ffconsultancy.com/products/index.html">Home Page</A>      
         </TD>
    <TD width="25%"><A href="http://www.ffconsultancy.com/products/fsharp_journal/subscribers/index.html">The 
      F# Journal</A>         </TD>
    <TD width="25%"></TR></TBODY></TABLE>
<TABLE id="page">
  <TBODY>
  <TR>
    <TD>
      <H1>Parallelizing the SciMark2 benchmark: part 1</H1>
      <P>The SciMark2 benchmark is one of the better benchmarks for programming 
      languages and their implementations in the context of technical computing. 
      This article is the first in a series revisiting the SciMark2 benchmark to 
      examine the potential for parallelizing each of its components. 
      Specifically, the Fast Fourier Transform (FFT) and Successive 
      Over-Relaxation (SOR) tasks are covered in this article. The results are 
      of general interest in the context of improving performance on multicores 
      using parallel programming.</P>
      <H2>Fast Fourier Transform (FFT)</H2>
      <P>The first of the five component tasks in the SciMark2 benchmark is a 
      familiar in-place Fast Fourier Transform (FFT) algorithm. Specifically, 
      the Danielson-Lanczos algorithm that bit reverses the order of the input 
      elements before performing a decimation-in-time FFT.</P>
      <P>The basic structure of this algorithm is as follows:</P>
<PRE>  let inline aux (data : _ array) i j wdr wdi =
    data.[j] &lt;- data.[i] - wdr
    data.[j+1] &lt;- data.[i+1] - wdi
    data.[i] &lt;- data.[i] + wdr
    data.[i+1] &lt;- data.[i+1] + wdi</PRE>
<PRE>  let transform_internal data d =
    bitreverse data
    let n = Array.length data / 2
    let mutable dual = 1
    while dual &lt; n do
      let mutable b = 0
      while b &lt; n do
        let i, j = 2 * b, 2 * (b + dual)
        aux data i j data.[j] data.[j + 1]
        b &lt;- b + 2 * dual
      let mutable wr = 1.
      let mutable wi = 0.
      let t = float d * pi / float dual
      let s, t = sin t, sin(t / 2.)
      let s2 = 2. * t * t
      for a=1 to dual-1 do
        let wr' = wr - s * wi - s2 * wr
        let wi' = wi + s * wr - s2 * wi
        wr &lt;- wr'
        wi &lt;- wi'
        let mutable b = 0
        while b &lt; n do
          let i, j = 2 * (b + a), 2 * (b + a + dual)
          let z1r, z1i = data.[j], data.[j+1]
          aux data i j (wr * z1r - wi * z1i) (wr * z1i + wi * z1r)
          b &lt;- b + 2 * dual
      dual &lt;- dual &lt;&lt;&lt; 1</PRE>
      <P>When looking through parallel eyes, there are two things to note about 
      this algorithm:</P>
      <P>
      <UL>
        <LI>The inner and outer loops cannot be interchanged in this algorithm 
        because the inner loops over                  <CODE>a</CODE> and         
                 <CODE>b</CODE> are dependent upon the variable                  
        <CODE>dual</CODE> of the outer loop.               </LI>
        <LI>The outer loop over                  <CODE>dual</CODE> cannot be 
        parallelized because iterations of its body are interdependent but the 
        inner loop over                  <CODE>a</CODE> can be parallelized.     
                  </LI></UL>
      <P></P>
      <P>Ideally, parallelism should only be exposed at the coarsest level (i.e. 
      the outermost loops should be parallelized) but the Danielson-Lanczos FFT 
      algorithm offers very little parallelism. Trying to parallelize this 
      algorithm by replacing the loop over              <CODE>a</CODE> with a    
                <CODE>Parallel.For</CODE> loop has disastrous consequences: 
      performance goes from 330MFLOPS down to only 4MFLOPS because most of the 
      time is spent handling parallel loops that have very little work to do.    
             </P>
      <P>Moreover, the array elements are indexed at              <CODE>2 * (b + 
      a)</CODE> so parallelizing the loop over              <CODE>a</CODE> will 
      introduce false sharing when              <CODE>b</CODE> is small because 
      different threads are likely to access neighboring array elements. This 
      problem is exascerbated by the current              
      <CODE>Parallel.For</CODE>  loop implementation which makes little attempt 
      to provide coherence between iterations and threads, i.e. subsequent 
      iterations are likely to be run on different threads.           </P>
      <P>These kinds of problems often occur in practice when trying to 
      parallelize real programs. In this case, the best solution is to use an 
      FFT algorithm that is more amenable to parallelization. The obvious choice 
      is the simpler radix-2 algorithm that uses a divide-and-conquer approach 
      to halve the input problem and, in particular, the FFTs of the halves may 
      be computed entirely independently. The radix-2 FFT may be implemented as 
      follows:</P>
<PRE>  let rec fft s (zs: _ array) =
    let n = zs.Length/2
    if n &gt; 1 then
      let zs1, zs2 = Array.zeroCreate n, Array.zeroCreate n
      for i=0 to n/2 - 1 do
        zs1.[2*i] &lt;- zs.[4*i]
        zs1.[2*i+1] &lt;- zs.[4*i+1]
        zs2.[2*i] &lt;- zs.[4*i+2]
        zs2.[2*i+1] &lt;- zs.[4*i+3]
      fft s zs1
      fft s zs2
      for k=0 to n/2 - 1 do
        let t = s * float k / float n
        let wr, wi = cos t, sin t
        let z1r, z1i, z2r, z2i = zs1.[2*k], zs1.[2*k+1], zs2.[2*k], zs2.[2*k+1]
        let z2r, z2i = z2r * wr - z2i * wi, z2r * wi + z2i * wr
        zs.[2*k] &lt;- z1r + z2r
        zs.[2*k+1] &lt;- z1i + z2i
        zs.[2*k+n] &lt;- z1r - z2r
        zs.[2*k+n+1] &lt;- z1i - z2i</PRE>
      <P>This serial implementation attains only 38MFLOPS compared to 330MFLOPS 
      for the Danielson-Lanczos algorithm. Thus, optimal performance requires 
      the use of this less efficient but parallelizable radix-2 algorithm for 
      the largest inputs resorting to the serial but significantly faster 
      Danielson-Lanczos algorithm when the size of the input falls below a 
      threshold.</P>
      <P>This may be implemented as follows:</P>
<PRE>  let rec fft s (zs: _ array) =
    let n = zs.Length/2
    if n &lt; (1 &lt;&lt;&lt; threshold) then
      danielson_lanczos zs s
    else
      if n &gt; 1 then
        let zs1, zs2 = Array.create n 0.0, Array.create n 0.0
        for i=0 to n/2 - 1 do
          zs1.[2*i] &lt;- zs.[4*i]
          zs1.[2*i+1] &lt;- zs.[4*i+1]
          zs2.[2*i] &lt;- zs.[4*i+2]
          zs2.[2*i+1] &lt;- zs.[4*i+3]
        let future = Tasks.Future.Create(fun () -&gt; fft s zs1)
        fft s zs2
        future.Value
        for k=0 to n/2 - 1 do
          let t = s * float k / float n
          let wr, wi = cos t, sin t
          let z1r, z1i, z2r, z2i = zs1.[2*k], zs1.[2*k+1], zs2.[2*k], zs2.[2*k+1]
          let z2r, z2i = z2r * wr - z2i * wi, z2r * wi + z2i * wr
          zs.[2*k] &lt;- z1r + z2r
          zs.[2*k+1] &lt;- z1i + z2i
          zs.[2*k+n] &lt;- z1r - z2r
          zs.[2*k+n+1] &lt;- z1i - z2i</PRE>
      <P>The SciMark2 benchmark provides both the default "small" suite of tests 
      as well as "large" tests. In the case of the FFT portion of the benchmark, 
      this equates to computing the FFTs of 1,024- or 1,048,576-element vectors, 
      respectively. Interestingly, the speedups obtained in these two cases are 
      quite different:</P>
      <P>
      <P style="text-align: center;"><IMG src="F%23%20Journal%20Parallelizing%20the%20SciMark2%20benchmark%20part%201_files/fft.gif"> 
                  </P>
      <P></P>
      <P>Note that parallelism is unable to improve performance at all in the 
      "small" case but the "large" case is up to 3.5× faster when leveraging 
      parallelism using this approach.</P>
      <P>Unlike previous examples where a single algorithm was parallelized, the 
      use of two separate FFT algorithms makes the interpretation of these 
      results non-trivial. One immediate observation is that the overheads of 
      parallelism are always larger than the benefits in the "small" case 
      because no speedup is ever achieved. However, this is not just because of 
      the time spent queueing and executing tasks but is now also because of the 
      extra time spent performing out-of-place FFTs that are much slower than 
      in-place FFTs using the Danielson-Lanczos algorithm.</P>
      <H2>Successive Over-Relaxation (SOR)</H2>
      <P>Ordinary SOR solvers conventionally use a pair of matrices, reading 
      from one and writing to the other before reversing their roles for the 
      next iteration. This algorithm is easily parallelized because iterations 
      of the loop body are independent and, consequently, the updates of all 
      matrix elements may be performed in parallel. This technique and its 
      straightforward parallelization were actually described  in the form of 
      the Gauss-Seidel method in the previous F#.NET Journal article             
       <I>"Simulating smoke in real-time using fluid dynamics"</I> (16th January 
      2008).           </P>
      <P>However, the solver described in the SciMark2 benchmark is unusual in 
      that it performs updates in-place and, therefore, introduces dependencies 
      between the order of updates to matrix elements:</P>
<PRE>&gt; let exec omega g i =
    let m, n = Array.length g, Array.length g.[0]
    for p=0 to i-1 do
      for i=1 to m-2 do
        for j=1 to n-2 do
          let s = g.[i-1].[j] + g.[i+1].[j] + g.[i].[j-1] + g.[i].[j+1]
          g.[i].[j] &lt;- 0.25 * omega * s + (1.0 - omega) * g.[i].[j];;
val exec2 : float -&gt; float [] [] -&gt; int -&gt; unit</PRE>
      <P>Specifically, matrix elements from the center (and not the left-most 
      column or top-most row) may only be updated once the elements to the left 
      and above have been updated. Suffice to say, these dependencies make the 
      algorithm much more interesting to parallelize.</P>
      <P>This serial algorithm takes 1.08s to apply 100 iterations to a 
      1,000×1,000 matrix:</P>
<PRE>&gt; let g =
    [|for i in 1 .. 1000 -&gt;
        [|for j in 1 .. 1000 -&gt;
            1 + i + j |&gt; float |&gt; sin|]|];;
Real: 00:00:00.301, CPU: 00:00:01.762, GC gen0: 3, gen1: 1, gen2: 1</PRE>
<PRE>val g : float [] [] =
  [|[|0.1411200081; -0.7568024953; -0.9589242747; -0.2794154982; 0.6569865987;
      0.9893582466; 0.4121184852; -0.5440211109; -0.9999902066; -0.536572918;
      0.4201670368; 0.9906073557; 0.6502878402; -0.2879033167; -0.9613974919;
      ...</PRE>
<PRE>&gt; exec 1.25 g 100;;
Real: 00:00:01.079, CPU: 00:00:01.092, GC gen0: 0, gen1: 0, gen2: 0
val it : unit = ()</PRE>
      <P>This performance result can be compared with the parallel routines 
      designed below.</P>
      <H3>Iterating concurrently</H3>
      <P>The simplest way to parallelize this in-place SOR implementation is to 
      note that iterations, represented by the variable              
      <CODE>p</CODE> in the original source code, can be performed concurrently. 
      That is not to say that the loop over              <CODE>p</CODE> may be 
      parallelized but, rather, that different iterations on different rows may 
      be performed concurrently. For example, once the first two rows have been 
      updated, the second iteration of the first row and the first iteration of 
      the third row may be computed simultaneously because they are independent. 
                </P>
      <P>The simplest way to leverage this available parallelism is to loop over 
      the even and odd rows of the matrix in parallel, updating them when the 
      elements above have been updated:</P>
<PRE>&gt; #r "System.Core.dll";;
--&gt; Referenced 'c:\Program Files\Reference Assemblies\Microsoft\Framework\v3.5\System.Core.dll'</PRE>
<PRE>&gt; #r "System.Threading.dll";;
--&gt; Referenced 'C:\Program Files\Microsoft Parallel Extensions Jun08 CTP\System.Threading.dll'</PRE>
<PRE>&gt; let exec2 omega g iters =
    let m, n = Array.length g, Array.length g.[0]
    let ps = Array.create m 0
    let row i =
      if (i=1 || ps.[i-1] = ps.[i] + 1) &amp;&amp; ps.[i] &lt; iters then
        for j=1 to n-2 do
          let s = g.[i-1].[j] + g.[i+1].[j] + g.[i].[j-1] + g.[i].[j+1]
          g.[i].[j] &lt;- 0.25 * omega * s + (1.0 - omega) * g.[i].[j]
        ps.[i] &lt;- ps.[i] + 1
    while ps.[m-2] &lt; iters do
      Parallel.For(1, m-1, 2, row)
      Parallel.For(2, m-1, 2, row);;
val exec2 : float -&gt; float [] [] -&gt; int -&gt; unit</PRE>
      <P>However, this parallel algorithm is 2× slower than the original series 
      algorithm:</P>
<PRE>&gt; let g2 =
    [|for i in 1 .. 1000 -&gt;
        [|for j in 1 .. 1000 -&gt;
            1 + i + j |&gt; float |&gt; sin|]|];;
Real: 00:00:00.301, CPU: 00:00:01.762, GC gen0: 3, gen1: 1, gen2: 1</PRE>
<PRE>val g2 : float [] [] =
  [|[|0.1411200081; -0.7568024953; -0.9589242747; -0.2794154982; 0.6569865987;
      0.9893582466; 0.4121184852; -0.5440211109; -0.9999902066; -0.536572918;
      0.4201670368; 0.9906073557; 0.6502878402; -0.2879033167; -0.9613974919;
      ...</PRE>
<PRE>&gt; exec2 1.25 g2 100;;
Real: 00:00:02.202, CPU: 00:00:15.132, GC gen0: 3, gen1: 0, gen2: 0
val it : unit = ()</PRE>
      <P>The problem is clearly that the overheads of parallelism are larger 
      than the benefits obtained from it. Fortunately, there is a simple 
      solution in this case: make tasks update several rows at a time.</P>
      <P>This leads to the following implementation:</P>
<PRE>&gt; let exec2 omega g iters =
    let m, n = Array.length g, Array.length g.[0]
    let ps = Array.create m 0
    let row i =
      if (i=1 || ps.[i-1] = ps.[i] + 1) &amp;&amp; ps.[i] &lt; iters then
        for j=1 to n-2 do
          let s = g.[i-1].[j] + g.[i+1].[j] + g.[i].[j-1] + g.[i].[j+1]
          g.[i].[j] &lt;- 0.25 * omega * s + (1.0 - omega) * g.[i].[j]
        ps.[i] &lt;- ps.[i] + 1
    while ps.[m-2] &lt; iters do
      Parallel.For(1, m-1, 2*step, fun i -&gt;
        for i=i to min (i+step-1) (m-2) do
          row i)
      Parallel.For(1+step, m-1, 2*step, fun i -&gt;
        for i=i to min (i+step-1) (m-2) do
          row i);;
val exec2 : float -&gt; float [] [] -&gt; int -&gt; unit</PRE>
      <P>Like the radix-2 FFT, this parallel implementation of the SOR benchmark 
      also obtains a modest but useful speedup on an 8 core machine:</P>
      <P>
      <P style="text-align: center;"><IMG src="F%23%20Journal%20Parallelizing%20the%20SciMark2%20benchmark%20part%201_files/sor.gif"> 
                  </P>
      <P></P>
      <P>Up to 2.25× faster than the serial implementation with step sizes in 
      the range 20-70.</P>
      <H2>Summary</H2>
      <P>This article has described the parallelization of two of the major 
      numerical algorithms presented in the SciMark2 benchmark: the Fast Fourier 
      Transform and Successive Over-Relaxation. These non-trivial algorithms 
      require significant effort to parallelize and the results show only modest 
      improvements in performance under certain circumstances. Both cases 
      require large problems to obtain significant performance improvements. 
      This demonstrates the difficulty of obtaining performance improvements on 
      multicore machines.</P>
      <P>Future F#.NET Journal articles will revisit the subject of parallelism, 
      including the parallelisation of the other three component algorithms that 
      make up the SciMark2 benchmark: Monte-Carlo, sparse matrix multiply and LU 
      decomposition with partial pivoting.</P></TD></TR></TBODY></TABLE>
<TABLE id="footer">
  <TBODY>
  <TR>
    <TD>© Flying Frog Consultancy Ltd., 2007</TD>
    <TD>Contact the            <A 
      href="mailto:webmaster@ffconsultancy.com">webmaster</A>         
  </TD></TR></TBODY></TABLE>
<SCRIPT src="F%23%20Journal%20Parallelizing%20the%20SciMark2%20benchmark%20part%201_files/urchin.js" type="text/javascript">
<script type="text/javascript">
_uacct = "UA-197840-1";
urchinTracker();</SCRIPT>
 </BODY></HTML>
